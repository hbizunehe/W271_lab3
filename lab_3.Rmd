---
title: 'Lab 3: Panel Models'
subtitle: 'US Traffic Fatalities: 1980 - 2004'
output:
    pdf_document: 
      toc: true
      toc_depth: 4
      number_sections: true
---

```{r load packages, echo=FALSE, message=FALSE}
list.of.packages <- c("plm", "grid", "ggrepel", "gridExtra", "tidyverse", "readr", "lubridate", "patchwork", "stargazer")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)

library(plm)
library(grid)
library(readr)
library(ggrepel)
library(gridExtra)
library(tidyverse)
library(lubridate)
library(patchwork)
library(stargazer)
```


\newpage

# U.S. traffic fatalities: 1980-2004

<!--
In this lab, we are asking you to answer the following **causal** question: 

> **"Do changes in traffic laws affect traffic fatalities?"**  

To answer this question, please complete the tasks specified below using the data provided in `data/driving.Rdata`. This data includes 25 years of data that cover changes in various state drunk driving, seat belt, and speed limit laws. 

Specifically, this data set contains data for the 48 continental U.S. states from 1980 through 2004. Various driving laws are indicated in the data set, such as the alcohol level at which drivers are considered legally intoxicated. There are also indicators for “per se” laws—where licenses can be revoked without a trial—and seat belt laws. A few economics and demographic variables are also included. The description of the each of the variables in the dataset is also provided in the dataset. 
-->

```{r load data, echo = TRUE}
load(file="./data/driving.RData")
```

# (30 points, total) Build and Describe the Data 

## (5 points) Load the data and produce useful features.

#### Status complete: Checked by Hannah

<!---
1. (5 points) Load the data and produce useful features. Specifically: 
    - Produce a new variable, called `speed_limit` that re-encodes the data that is in `sl55`, `sl65`, `sl70`, `sl75`, and `slnone`; 
    - Produce a new variable, called `year_of_observation` that re-encodes the data that is in `d80`, `d81`, ... , `d04`. 
    - Produce a new variable for each of the other variables that are one-hot encoded (i.e. `bac*` variable series). 
    - Rename these variables to sensible names that are legible to a reader of your analysis. For example, the dependent variable as provided is called, `totfatrte`. Pick something more sensible, like, `total_fatalities_rate`. There are few enough of these variables to change, that you should change them for all the variables in their data. (You will thank yourself later.)
-->



```{r reencode}
# For the fractions, we are taking the majority as a speed limit
# We skipped year_of_observation since there a year column which aligns with dx
df <- data %>%
  mutate(speed_limit = ifelse(sl55 >= 0.5, '55',
                       ifelse(sl65 >= 0.5, '65',
                       ifelse(sl70 >= 0.5, '70',
                       ifelse(sl75 >= 0.5, '75',
                       ifelse(slnone >= 0.5, 'none', '0')
                       ))))) %>%
           mutate(speed_limit=factor(speed_limit, 
                  levels=c('55', '65', '70', '75', 'none')),
         blood_alcohol_limit_10 = ifelse(bac10 >= 0.5, 1, 0),
         blood_alcohol_limit_08 = ifelse(bac08 >= 0.5, 1, 0)) %>% 
  mutate(bac=ifelse(blood_alcohol_limit_10==1, '10', 
             ifelse(blood_alcohol_limit_08==1, '8', 'none'))) %>%
  mutate(bac=factor(bac, levels=c('none', '10', '8'))) %>%
  select(!c((sl55:slnone), (d80:d04), bac10, bac08)) %>% # Excluding
  rename(minimum_drinking_age = minage, zero_tolerance_law = zerotol,
         graduated_drivers_license_law = gdl, per_se_law = perse,
         total_fatalities = totfat, nighttime_fatalities = nghtfat,
         weekend_fatalities = wkndfat, total_fatalities_per_100M_miles = totfatpvm,
         nighttime_fatalities_per_100M_miles = nghtfatpvm,
         weekend_fatalities_per_100M_miles = wkndfatpvm,
         state_population =  statepop, total_fatalities_rate = totfatrte,
         nighttime_fatalities_rate = nghtfatrte,
         weekend_fatalities_rate = wkndfatrte,
         vehicle_miles_traveled = vehicmiles, unemployment_rate = unem,
         population_aged_14_to_24_rate = perc14_24,
         speed_limit_70_plus = sl70plus, 
         seat_belt = seatbelt,
         primary_seatbelt_law = sbprim, secondary_seatbelt_law = sbsecon,
         miles_driven_per_capita = vehicmilespc) %>%
  mutate(speed_limit_70_plus = 
           ifelse(speed_limit_70_plus>0.5, 1, 0)
           ) %>%
  mutate(seat_belt_law = 
           ifelse(seat_belt==0, 'none',
           ifelse(seat_belt==2, 'secondary', 
           ifelse(seat_belt==1, 'primary', 'na')))) %>%
  mutate(seat_belt_law=factor(seat_belt_law, 
                              levels=c('none', 'secondary', 'primary')),
         ) %>%
  mutate(per_se_law=round(per_se_law, 0)) %>%
  mutate(per_se_law=factor(per_se_law, levels=c(0, 1))) %>%
  mutate(log_total_fatalities_rate = log10(total_fatalities_rate))
  

# Adding states to the dataframe
state_df <- data.frame("index" = 1:51,
                       "state_name" = sort(c(state.name, "District of Columbia")))
main_df <- merge(df, state_df, by.x = 'state', by.y ='index')

pdata <- pdata.frame(main_df, index=c("state", "year"))
head(main_df)
```

## (5 points) Provide a description of the basic structure of the dataset. 

#### Status Complete: Checked by Hannah

<!--
2. (5 points) Provide a description of the basic structure of the dataset. What is this data? How, where, and when is it collected? Is the data generated through a survey or some other method? Is the data that is presented a sample from the population, or is it a *census* that represents the entire population? Minimally, this should include:
    - How is the our dependent variable of interest `total_fatalities_rate` defined? 
-->

The data contains state-level information on the total traffic fatalities rate in a specific year. The data also contains state law information that could have an impact on the traffic fatalities rate, like the speed limit, blood alcohol legal limit, seat belt laws, etc. The data is likely a census that represents the entire population since it's aggregated on a yearly level. The outcome of interest, `total_fatalities_rate` is defined as the number of fatalities per 100,000 people.

##  (20 points) EDA

#### Status Complete: Add descriptions

<!--
3. (20 points) Conduct a very thorough EDA, which should include both graphical and tabular techniques, on the dataset, including both the dependent variable `total_fatalities_rate` and the potential explanatory variables. Minimally, this should include: 
    - How is the our dependent variable of interest `total_fatalities_rate` defined? 
    - What is the average of `total_fatalities_rate` in each of the years in the time period covered in this dataset? 

As with every EDA this semester, the goal of this EDA is not to document your own process of discovery -- save that for an exploration notebook -- but instead it is to bring a reader that is new to the data to a full understanding of the important features of your data as quickly as possible. In order to do this, your EDA should include a detailed, orderly narrative description of what you want your reader to know. Do not include any output -- tables, plots, or statistics -- that you do not intend to write about.
-->


```{r eda plots, fig.height=7, echo=FALSE}
# Variable name to actual name mapping that will used in the plot
name.map <- list(log_total_fatalities_rate = 'Log total fatality rate',
                 speed_limit = 'Speed limit', bac = 'BAC',
                 speed_limit_70_plus = 'Speed limit 70 plus',
                 seat_belt_law = 'Seat belt law',
                 graduated_drivers_license_law = 'Graduated drivers license law',
                 per_se_law = 'Per se law',
                 population_aged_14_to_24_rate = 'Aged 14 to 24 rate',
                 miles_driven_per_capita = 'Miles driven per capita',
                 unemployment_rate = 'Unemployment rate',
                 minimum_drinking_age = 'Minimum drinking age')

# Time series line plot
# Variable to plot as var_name
# Each line gets a color based on the group_name
# X-axis is always year
plots.ts.by.group <- function(df, var_name, group_name, subtitle='') {
  plt <- df %>% 
    mutate(!!group_name := factor(df[[group_name]])) %>% 
      ggplot(aes_string(x='year', y=var_name, group='state')) + 
      geom_line(aes_string(color=group_name)) +
    labs(subtitle=subtitle, x = 'Year', y=name.map[[var_name]],
         color=name.map[[group_name]]) +
    theme(legend.position = c(0.8, 0.8), legend.key.size = unit(0.2, "cm"), 
          legend.text=element_text(size=rel(0.5)), 
          legend.title=element_text(size=rel(0.7)))
    return((plt))
}

# Box plot by group
# Variable to plot as var_name
# Each group is plit by group_name
plots.box.by.group <- function(df, var_name, group_name, subtitle='')  {
  plt <- df %>% mutate(factored=factor(df[[group_name]])) %>%
    ggplot(aes_string(x='factored', y=var_name)) +
    geom_boxplot(outlier.colour="red", outlier.shape=8,  outlier.size=4) +
    theme(axis.text.x=element_text(angle=-90)) +
    labs(subtitle=subtitle, x=name.map[[group_name]], y=name.map[[var_name]])
    return((plt))
}

# Time series line plot
# Variable to plot as var_name
# Each line gets a color based on the group_name
# X-axis is always year
plots.scatter.by.state <- function(df, var_name, subtitle='') {
plt <- df  %>%
    ggplot(aes_string(x=var_name, y='log_total_fatalities_rate')) +
    geom_point(aes(color=state_name)) + 
  geom_smooth(method = lm, formula = y ~ x) + 
  labs(subtitle=subtitle, x=name.map[[var_name]],
       y=name.map$log_total_fatalities_rate) +
  theme(legend.position = 'none')
  return((plt))
}
```

### Average and state wide trends

#### Status Complete: Add descriptions.

```{r overall mean EDA, echo=FALSE, fig.align='center', fig.height=4, fig.width=10}
# Average mean for total_fatalities_rate over years
main_plt <- main_df %>%
  group_by(year) %>%
  summarise(mean = mean(total_fatalities_rate), n = n()) %>%
  ggplot(aes(x = year, y = mean)) +
  geom_line() +
  labs(title = "Average mean fatality rate across US",
       subtitle = "Fatality rate is going down",
       x = "Year",  y = "Fatality Rate") +
  theme(legend.position = "none")

main_plt
```

In the plot shown above, the fatality rates across states have been aggregated and the average fatality rate is plotted against time. The plot shows that, over time, average fatality rate has decreased in the United States, beginning at an average of approximately 25 fatalities in 1980 to an average of about 17 fatalities in 2004, per 100,000 people.

However, given that the data is aggregated across states and the fact that the driving conditions and laws vary significantly across states is common knowledge, the graph shown above enables only a limited understanding of the trend in traffic fatalities across the United States over time. 


```{r, fatality rate by state EDA, fig.height=6, fig.width=12, echo=FALSE}
cut_point <- c(-1, 12, 24, 36, 48)
plots <- vector('list', 4)

for (i in 2:5) {
  plots[[i-1]] <- (pdata %>%
    filter(as.integer(state) > cut_point[i-1] & as.integer(state) <= cut_point[i]) %>%
    ggplot(aes(x = as.Date(year,"%Y"), y = total_fatalities_rate)) +
    geom_line() +
    facet_wrap(~ state_name, nrow = 3, ncol=4) +
    labs(x = "Year",  y = "Total Fatalities Rate") +
    theme(legend.position = "none"))
}

grid.arrange(plots[[1]], plots[[2]], plots[[3]], plots[[4]], nrow = 2, ncol = 2,
             top = textGrob("Fatality rate by states", gp=gpar(fontsize=20)))
```
Splitting the data by state provides the additional context needed to understand the trend in traffic fatalities. For most states, as is expected, the fatality rates decreased over the years. However, for some states, like Alabama and Arkansas, the trend does not show much change over time. Mississipi, on the other hand, shows an increase in fatality rate over time.


```{r, echo=FALSE, fig.align='center', fig.height=4, fig.width=10}
without.tran <- main_df %>%
  ggplot(aes(x = total_fatalities_rate)) +
    geom_density(color = 'grey', fill = 'grey') +
    labs(x='Total Fatalities Rate', y = 'Density')

with.tran <- main_df %>% 
  ggplot(aes(x = log_total_fatalities_rate)) +
  geom_density(color = 'grey', fill = 'grey') +
    labs(x='Log of Total Fatalities Rate', y = 'Density')

(without.tran | with.tran) +
  plot_annotation(title = 'Log Transform Normalizes Total Fatalities Rate')
```

The `total_fatalities_rate` variable follows a right-skewed distribution. In the plot shown above, we see that the log-transformation helps make the distribution more normal

```{r,  fig.align='center', fig.width=10, fig.height=12, echo=FALSE, warning=FALSE}
# Speed limit
plt.ts.sl = plots.ts.by.group(
  main_df, 'log_total_fatalities_rate', 'speed_limit',
  subtitle='Traffic fatalities in state colored by speed limit')

tmp_df <- main_df %>% filter(year>1987)
plt.box.sl = plots.box.by.group(
  tmp_df, 'log_total_fatalities_rate', 'speed_limit',
  subtitle='Fatalities comparison for year > 1987')

# Speed limit
plt.ts.sl70p = plots.ts.by.group(
  main_df, 'log_total_fatalities_rate', 'speed_limit_70_plus',
  subtitle='Traffic fatalities in state colored by speed limit > 70mph')

tmp_df <- main_df %>% filter(year>1995)
plt.box.sl70p = plots.box.by.group(
  tmp_df, 'log_total_fatalities_rate', 'speed_limit_70_plus',
  subtitle='Fatalities comparison for year > 1987')

# Seat belt law
plt.ts.sb = plots.ts.by.group(
  main_df, 'log_total_fatalities_rate', 'seat_belt_law',
  subtitle='Traffic fatalities in state colored by seat belt laws')

tmp_df <- main_df %>% filter(year>1987)
plt.box.sb = plots.box.by.group(
  tmp_df, 'log_total_fatalities_rate', 'seat_belt_law',
  subtitle='Fatalities comparison for year > 1987')

# Graduated drivers licence law
tmp_df <- main_df %>% mutate(
  graduated_drivers_license_law=round(graduated_drivers_license_law, 0))
plt.ts.gdl = plots.ts.by.group(tmp_df, 
  'log_total_fatalities_rate', 'graduated_drivers_license_law',
  subtitle='Traffic fatalities in state colored by graduated drivers licence laws')

tmp_df <- tmp_df %>% filter(year>1995)
plt.box.gdl = plots.box.by.group(
  tmp_df, 'log_total_fatalities_rate', 'graduated_drivers_license_law',
  subtitle='Fatalities comparison for year > 1995')

# Per-Se law
tmp_df <- main_df
plt.ts.ps = plots.ts.by.group(tmp_df, 
  'log_total_fatalities_rate', 'per_se_law',
  subtitle='Traffic fatalities in state colored by Per-se law')

tmp_df <- tmp_df %>% filter(year>1990)
plt.box.ps = plots.box.by.group(
  tmp_df, 'log_total_fatalities_rate', 'per_se_law',
  subtitle='Fatalities comparison for year > 1990')

# BAC levels
plt.ts.bac = plots.ts.by.group(
  main_df, 'log_total_fatalities_rate', 'bac',
  subtitle='Traffic fatalities in state colored by BAC level')
plt.box.bac = plots.box.by.group(
  main_df %>% filter(year>1990), 'total_fatalities_rate', 'bac',
  subtitle='Fatalities comparison for year > 1990')


(plt.ts.sl | plt.box.sl)/(plt.ts.sl70p | plt.box.sl70p)/
 (plt.ts.sb | plt.box.sb)/(plt.ts.gdl | plt.box.gdl)/
  (plt.ts.ps | plt.box.ps) /(plt.ts.bac | plt.box.bac) +
  plot_annotation(title='Time series and distribution comparisons of factor variables')
```

> TODO: Add takeaways from above graph

### Description of variables

#### Status Complete: Format descriptions.

#### Factor variables

* The top-left panel shows the relationship between the highway speed limit and the logged `total_fatalities_rate` variable. Before 1987, the highway speed limit was uniformly set to 55mph across all states. Since then, different states have adopted different speed limits. In 1997,  there was a significant increase in highway speeds across multiple states. The box plot in the top-right panel compares the fatality rate across different speed limits  filtered for years greater than 1987. We see that increasing speed limits are associated with increased fatality rates. As there are states with no speed limit, this variable has been treated as a factor. 
* Thresholding speed limits for greater than or lower than 70mph (second panel from the top) shows a similar pattern of higher speeds being associated with higher fatality rates.
* Seat belts started to become mandatory, across states, beginning mid-to-late 80s, and today there is only one state which does not mandate seat belts. Primary laws are the strictest and allow police to ticket drivers and passengers who are not wearing a proper safety restraint, even if that is the only traffic violation they are committing. Secondary seat belt laws, on the other hand, do not grant law enforcement officials the right to ticket drivers or passengers for failing to wear a safety restraint unless another traffic violation has occurred. There are 15 states with secondary seat belt laws. Source: https://www.cooper-law-firm.com/what-is-the-difference-between-primary-and-secondary-seat-belt-laws/. 
* The graduated drivers licence law was beginning to be introduced in the late 90's. The box plot, which has been filtered for years greater than 1995, suggests that even for that time frame, there is a reduction in fatality rate between the two groups.
* Some states had Per-Se laws before the start of the data in 1980 and some still did not have Per-Se laws in 2004. There is a gradual increase in the adoption of the law from 1980 to about the 2000s. Surprisingly, there is an increase in fatality rates in comparison of data with the Per-Se law as compared to without it. 
* Lastly, most states had adopted a blood alcohol limit by the mid 80s, with two states choosing a limit only in 2002. 

```{r, fig.width=8, fig.height=9, echo=FALSE}
# population_aged_14_to_24_rate
tmp_df <- main_df %>% mutate(population_aged_14_to_24_rate=
                               round(population_aged_14_to_24_rate/3, 0) *3)
plt.ts.perc = plots.ts.by.group(
  tmp_df, 'log_total_fatalities_rate', 'population_aged_14_to_24_rate',
  subtitle='Traffic fatalities by age 14 to 24 rate')

tmp_df <- main_df %>% filter(year>1980)
plt.scatter.perc = plots.scatter.by.state(
  main_df, 'population_aged_14_to_24_rate',
  subtitle='Aged 14 to 24 rate relationship to fatality rate')

# Miles driven per capita
tmp_df <- main_df %>% mutate(miles_driven_per_capita=
                               round(miles_driven_per_capita/2000, 0)*2000)
plt.ts.mpc = plots.ts.by.group(
  tmp_df, 'log_total_fatalities_rate', 'miles_driven_per_capita',
  subtitle='Traffic fatalities by miles driven per capita')

plt.scatter.mpc = plots.scatter.by.state(
  main_df, 'miles_driven_per_capita',
  subtitle='Miles driven per capita relationship to fatality rate')

# Unemployment rate
tmp_df <- main_df %>% mutate(unemployment_rate=round(unemployment_rate/3, 0)*3)

plt.ts.ur = plots.ts.by.group(
  tmp_df, 'log_total_fatalities_rate', 'unemployment_rate',
  subtitle='Traffic fatalities by unemployment rate')

plt.scatter.ur = plots.scatter.by.state(
  main_df, 'unemployment_rate',
  subtitle='Unemployment rate relationship to fatality rate')

# Minimum drinking age
tmp_df <- main_df %>% mutate(minimum_drinking_age=round(minimum_drinking_age, 0))
plt.ts.mda = plots.ts.by.group(
  tmp_df, 'log_total_fatalities_rate', 'minimum_drinking_age',
  subtitle='Traffic fatalities by minimum drinking age')

plt.scatter.mda = plots.scatter.by.state(
  main_df, 'minimum_drinking_age',
  subtitle='Minimum drinking age relationship to fatality rate')


(plt.ts.perc | plt.scatter.perc) /  (plt.ts.mpc | plt.scatter.mpc) /
  (plt.ts.ur | plt.scatter.ur) / (plt.ts.mda | plt.scatter.mda) +
  plot_annotation(title='Time series and correlation comparison of continous variables colored by variable values')
```


#### Description of Continous Variables

* There is a decrease in the percentage of 14 to 24 year-olds in the population over time. This is correlated to the decrease in the fatalities during that time period.
* Miles driven per capita has a positive relationship with total fatality rate. An increase in miles driven is associated with an increase in fatality rate 
* There is an increase in fatality rate with an increase in unemployment rate. 
* The minimum drinking age has been 21 in most states since the late 80s. There is a general decrease in fatality  rate with an increased minimum drinking age but there also has been a general decrease in fatality rates during the time period when the age limits were changed. 

# (15 points) Preliminary Model

#### Status complete: Checked by Hannah

<!--
Estimate a linear regression model of *totfatrte* on a set of dummy variables for the years 1981 through 2004 and interpret what you observe. In this section, you should address the following tasks: 

- Why is fitting a linear model a sensible starting place? 
- What does this model explain, and what do you find in this model? 
- Did driving become safer over this period? Please provide a detailed explanation.
- What, if any, are the limitation of this model. In answering this, please consider **at least**: 
    - Are the parameter estimates reliable, unbiased estimates of the truth? Or, are they biased due to the way that the data is structured?
    - Are the uncertainty estimate reliable, unbiased estimates of sampling based variability? Or, are they biased due to the way that the data is structured? 
-->

```{r}
#  Pooled OLS model
pooled_ols <- plm(log_total_fatalities_rate ~ year, data = pdata,
                  index = c("state", "year"),
                  effect = "individual", model = "pooling")
summary(pooled_ols)
```

* 'Starting from a linear model gives us an easy and intuitive way to understand the overall trend pattern over the years via the coefficients on the year dummy variables. In later sections, the report details panel data analysis that can be compared against this benchmark.'  
* 'This model explains the log of fatality rate over different years compared to the baseline year, which is 1980. All coefficients except for the 1981 year are statistically significant. From this model, we learn that in all years following the baseline up to 2004, the fatality rate goes down compared to the baseline year 1980. The decline accelerates as the year increases but not consistently, as seen, for example, between 1986 - 1988, where the fatality rate increases compared to previous years. However, it returns to the decline track/trend in 1989. The higher, in absolute value, coefficients over the years means that driving becomes safer as the fatality rate keeps decreasing. For example, the fatality rate, which was ```r round(10^coef(pooled_ols)['(Intercept)'], 2)``` as of 1980, became ```r round(10^coef(pooled_ols)['year1990'], 2)``` in 1990, ```r round(10^coef(pooled_ols)['year2000'], 2)``` in 2000, and ```r round(10^coef(pooled_ols)['year2004'], 2)``` in 2004, showing that driving over the years has become safer. In other words, ```r(round(10^coef(pooled_ols)['(Intercept)'] - 10^(coef(pooled_ols)['(Intercept)']+coef(pooled_ols)['year2004']), 2))``` fewer people are predicted to be involved in traffic fatalities out of 100,000 people in 2004 than in 1980 as per this model.'   
* 'Note that we are ignoring unobserved Heterogeneity and the group structure by taking each entry as a separate observation. Because of that, residuals generally correlate across time and have heteroskedasticity across and/or within groups. Heteroscedastic residuals are a violation of the OLS Homoscedasticity assumption, which will make it difficult to trust the standard error. As a result, the confidence interval can not be trusted as it can be too wide or narrow. Also, the independence assumption (no autocorrelation) is violated since we did not accommodate the lag/trend component, which makes the OLS estimates to be unreliable; in other words, our OLS estimator is not the Best Linear Unbiased Estimator.'   

# (15 points) Expanded Model 

#### Status complete: Checked by Hannah

<!--
Expand the **Preliminary Model** by adding variables related to the following concepts: 

- Blood alcohol levels 
- Per se laws
- Primary seat belt laws (Note that if a law was enacted sometime within a year the fraction of the year is recorded in place of the zero-one indicator.)
- Secondary seat belt laws 
- Speed limits faster than 70 
- Graduated drivers licenses 
- Percent of the population between 14 and 24 years old
- Unemployment rate
- Vehicle miles driven per capita. 

If it is appropriate, include transformations of these variables. Please carefully explain carefully your rationale, which should be based on your EDA, behind any transformation you made. If no transformation is made, explain why transformation is not needed. 

- How are the blood alcohol variables defined? Interpret the coefficients that you estimate for this concept.
- Do *per se laws* have a negative effect on the fatality rate?
- Does having a primary seat belt law?
-->

## Transformation (treatment) of Variables
* As described in a previous section, a log transformation was performed on the response variable to make the distribution closer to normal.
* States where highway speeds were made over 70mph during the middle of the year contained a fractional value for that year. The fraction was thresholded at 0.5 to make this a binary variable. As there is no meaningful interpretation for this variable as a continuous value, this threshold was necessary to convert it into a factor.
* A state can have a primary, secondary or no seat belt laws. The seat_belt_law reflects these three factors combined into one variable. 
* Graduated drivers licence law and per-se law parameters both have fractional value for years where the law was implemented mid-year. The fractions were thresholded at 0.5 to make this a binary variable. As there is no meaningful interpretation for these variables as a continuous value, this threshold was necessary to convert them into a factor.
* Although Blood alcohol content (BAC) levels of 0.08% or 0.1% lends itself to a numeric interpretation, there is no numeric value associated with no BAC limit. For this reason BAC has been treated as a factor variable with levels none, 10 and 8.
* None of the continuous variables, namely, unemployment rate, miles drive per-capita, proportion  of 14 to 24 aged people in population required any transformation. This can be seen in the correlation plots in Figure y where their relationship to log fatality rates appear linear.

### Pooled OLS Model Creation

```{r expanded model}
expanded.ols.data <- main_df %>% select(
  c(log_total_fatalities_rate, bac, per_se_law, seat_belt_law, 
    graduated_drivers_license_law, population_aged_14_to_24_rate,
    minimum_drinking_age, unemployment_rate, speed_limit_70_plus, 
    miles_driven_per_capita, year, state)) 

main_p <- pdata.frame(expanded.ols.data, index=c("state", "year"))
expanded.ols <- plm(log_total_fatalities_rate ~  year + bac + 
                      population_aged_14_to_24_rate + miles_driven_per_capita + 
                      unemployment_rate + speed_limit_70_plus + per_se_law + 
                      seat_belt_law + graduated_drivers_license_law,
                    data = main_p,
                  index = c("state", "year"),
                  effect = "individual", model = "pooling")

summary(expanded.ols)
```

## Interpretation of Results 

###  How are the blood alcohol variables defined? Interpret the coefficients that you estimate for this concept. 

* In an earlier section we have defined our treatment of the BAC variable as a factor with levels 'none', '10' and '8'. In this model we note that the base level is no blood alcohol limit. We note that bac value of 0.08% is statistically significant and 0.1% is not statistically significant. The model suggests that setting a blood alcohol limit of 0.1% is associated with, ceteris paribus, a ```r round(10^coef(expanded.ols)['bac10'], 2)``` times decrease in fatality rate as compared to no BAC limit.  The model suggests that setting a blood alcohol limit of 0.08% is associated with, ceteris paribus, a ```r round(10^coef(expanded.ols)['bac8'], 2)``` times decrease in fatality rate as compared to no BAC limit.
    
### Do *per se laws* have a negative effect on the fatality rate? 

* We note that per-se law is not a statistically significant parameter. The model suggests that having a per-se law is associated with, ceteris paribus, a ```r round(10^coef(expanded.ols)['per_se_law1'], 2)``` times decrease in fatality rate as compared to not having per-se law.

### Does having a primary seat belt law reduce fatality rates?

* We note that the seat belt law factors are not statistically significant. We also note that the implementation of the primary law leads to, ceteris paribus, a  ```r round(10^coef(expanded.ols)['seat_belt_lawprimary'], 3)``` times decrease in fatality rate which is practically insignificant. 

# (15 points) State-Level Fixed Effects 

#### Status complete: Checked by Hannah

## Re-estimate the **Expanded Model** using fixed effects at the state level. 

<!--
Re-estimate the **Expanded Model** using fixed effects at the state level. 

- What do you estimate for coefficients on the blood alcohol variables? How do the coefficients on the blood alcohol variables change, if at all? 
- What do you estimate for coefficients on per se laws? How do the coefficients on per se laws change, if at all? 
- What do you estimate for coefficients on primary seat-belt laws? How do the coefficients on primary seatbelt laws change, if at all? 

Which set of estimates do you think is more reliable? Why do you think this? 

- What assumptions are needed in each of these models?  
- Are these assumptions reasonable in the current context?
-->

Model estimation for a fixed effect (within) model.

```{r within model}
expanded.within <- plm(log_total_fatalities_rate ~  bac + year  +
                      population_aged_14_to_24_rate + miles_driven_per_capita + 
                      unemployment_rate + speed_limit_70_plus + per_se_law + 
                      seat_belt_law + graduated_drivers_license_law,
                    data = main_p,
                  index = c("state", "year"),
                  effect = "individual", model = "within")

summary(expanded.within)
```

## Interpretation of the Model

```{r, echo=FALSE, warning=FALSE, results = 'asis'}
stargazer(expanded.ols, expanded.within,
          keep=c("bac10", "bac8", "per_se_law1", "seat_belt_lawprimary"),
          style="qje", omit.stat=c("adj.rsq","f"), header = FALSE,
          column.labels = c( "Pooled OLS", "Within"))
```


### What do you estimate for coefficients on the blood alcohol variables? How do the coefficients on the blood alcohol variables change, if at all? 

We note that BAC value of 0.1% and 0.08% are both statistically significant parameters. 

  * This model suggests that setting a blood alcohol limit of 0.1% is associated with, ceteris paribus, a ```r round(10^coef(expanded.within)['bac10'], 2)``` times decrease in log fatality rate as compared to no BAC limit. Compared to the OLS model which showed that setting a blood alcohol limit of 0.1% is associated with, ceteris paribus, a ```r round(10^coef(expanded.ols)['bac10'], 2)``` times decrease in fatality rate as compared to no BAC limit. 
  * This model suggests that setting a blood alcohol limit of 0.08% is associated with, ceteris paribus, a ```r round(10^coef(expanded.within)['bac8'], 2)``` times decrease in fatality rate as compared to no BAC limit. Compared to the OLS model which showed that setting a blood alcohol limit of 0.08% is associated with, ceteris paribus, a ```r round(10^coef(expanded.within)['bac8'], 2)``` times decrease in fatality rate as compared to no BAC limit.


### What do you estimate for coefficients on per se laws? How do the coefficients on per se laws change, if at all? 

We note that per-se law is a statistically significant parameter. This model suggests that having a per-se law is associated with, ceteris paribus, a ```r round(10^coef(expanded.within)['per_se_law1'], 2)``` times decrease in fatality rate as compared to not having per-se law. Compared to the OLS model which showed that having a per-se law is associated with, ceteris paribus, a ```r round(10^coef(expanded.ols)['per_se_law1'], 2)``` times decrease in fatality rate as compared to not having per-se law

### What do you estimate for coefficients on primary seat-belt laws? How do the coefficients on primary seatbelt laws change, if at all?

In this model we note that seat belt primary law is a significant factor. This model suggests that having a primary seat belt law is associated with, ceteris paribus, a ```r round(10^coef(expanded.within)['seat_belt_lawprimary'], 2)``` unit decrease in fatality rate as compared to not having  primary seat belt law. Compared to the OLS model for which the parameter was not statistically significant and had a close to 0 value of ```r round(10^coef(expanded.ols)['seat_belt_lawprimary'], 2)```

## Model Assumptions

A fundamental assumption in an pooled OLS model is that the data is IID. Let's consider a data set where a sample of a large population was collected on different years. It is unlikely that a particular individual sample is measured twice. In such a circumstance a pooled OLS model would be applicable. However in this data set, the individual is the state and the same state is measured multiple times across years. This violates the assumption of IID in the pooled OLS. A fixed effect model is then expected to be a better model in this scenario. We perform a F-test between the pooled and the fixed effect model to check for fixed effects. The null hypothesis is that there are no fixed effects and the alternate hypothesis is that there are fixed effects. We test against an alpha of 0.05

```{r}
res <- pFtest(expanded.within, expanded.ols)
```

With a p-value of `r as.numeric(res$p.value)` less than 0.05, we reject the null hypothesis of no fixed effects. This means we should include state and/or time fixed effects in our model. Hence the fixed effect model is better for this scenario.

# (10 points) Consider a Random Effects Model 

#### Status Mostly Complete: Include consequences about inappropriately estimating a random effects model.

<!--
Instead of estimating a fixed effects model, should you have estimated a random effects model?

- Please state the assumptions of a random effects model, and evaluate whether these assumptions are met in the data. 
- If the assumptions are, in fact, met in the data, then estimate a random effects model and interpret the coefficients of this model. Comment on how, if at all, the estimates from this model have changed compared to the fixed effects model. 
- If the assumptions are **not** met, then do not estimate the data. But, also comment on what the consequences would be if you were to *inappropriately* estimate a random effects model. Would your coefficient estimates be biased or not? Would your standard error estimates be biased or not? Or, would there be some other problem that might arise?
-->

## Random Effect Model Assumptions

Assumption for a random effects model is $cov(x_{it}, a_i) = 0$, i.e. the fixed effect is uncorrelated to the independent variables at any time period.

## Estimating the Random Effect Model

```{r random model}
expanded.re <- plm(log_total_fatalities_rate ~  bac + year  +
                      population_aged_14_to_24_rate + miles_driven_per_capita + 
                      unemployment_rate + speed_limit_70_plus + per_se_law + 
                      seat_belt_law + graduated_drivers_license_law,
                    data = main_p,
                  index = c("state", "year"),
                  model = "random")

summary(expanded.re)
```

We conduct a Hausman test for random vs. fixed effects using `phtest`. We perform this test with an $\alpha = 0.05$

```{r}
res <- phtest(expanded.within, expanded.re)
```

With a p-value of `r res$p.value` much less than $\alpha$, we reject the null hypothesis that random effects are appropriate, suggesting that we should use the fixed models. The random effects model is not likely to be consistent in this case.

> TODO: Comment about consequences

# (10 points) Model Forecasts 

#### Status Completed: Checked by Hannah

<!--
The COVID-19 pandemic dramatically changed patterns of driving. Find data (and include this data in your analysis, here) that includes some measure of vehicle miles driven in the US. Your data should at least cover the period from January 2018 to as current as possible. With this data, produce the following statements: 

- Comparing monthly miles driven in 2018 to the same months during the pandemic: 
  - What month demonstrated the largest decrease in driving? How much, in percentage terms, lower was this driving? 
  - What month demonstrated the largest increase in driving? How much, in percentage terms, higher was this driving? 
  
Now, use these changes in driving to make forecasts from your models. 

- Suppose that the number of miles driven per capita, increased by as much as the COVID boom. Using the FE estimates, what would the consequences be on the number of traffic fatalities? Please interpret the estimate.
- Suppose that the number of miles driven per capita, decreased by as much as the COVID bust. Using the FE estimates, what would the consequences be on the number of traffic fatalities? Please interpret the estimate.
-->

```{r comparing monthly miles driven}
# Source
# U.S. Federal Highway Administration, Vehicle Miles Traveled [TRFVOLUSM227NFWA],
#   retrieved from FRED, Federal Reserve Bank of St. Louis;
# https://fred.stlouisfed.org/series/TRFVOLUSM227NFWA, November 29, 2022.
# Driving mile units in millions
miles.driven <- 
  readr::read_csv("./data/TRFVOLUSM227NFWA.csv", show_col_types = FALSE) %>%
  rename(miles = TRFVOLUSM227NFWA, date = DATE)

largest.decrease <- 0
largest.increase <- 0
max <- -Inf
min <- Inf

# Comparing each month miles driven
for (i in 1:12) {
  val.18 <- miles.driven %>%
    filter(date == mdy(paste(i, 1, 2018, sep="/")))
  
  # Comparing 2018 with 2020 and 2021
  for (j in 2020:2021) {
    val.covid <- miles.driven %>%
      filter(date == mdy(paste(i, 1, j, sep="/")))

    diff <- val.covid$miles - val.18$miles
    if(diff < min) {
      largest.decrease <- mdy(paste(i, 1, j, sep="/"))
      min = diff
    }
    if(diff > max) {
      largest.increase <- mdy(paste(i, 1, j, sep="/"))
      max = diff
    }
  }
}

# Calculating percentage
prev <- miles.driven %>%
    filter(date == largest.decrease)
lower.percentage  <- min/prev$miles

prev <- miles.driven %>%
    filter(date == largest.increase)
higher.percentage <- max/prev$miles
```

The largest month-over-month decrease in driving was in `r format(as.Date(largest.decrease), "%B %Y")` and it was lower by `r round(lower.percentage * 100, 2)`%. This was right as the pandemic was starting to make its way through the country and the response was to shut everything down. So a rapid decrease in driving would make sense. In contrast, the largest month-over-month increase in driving was in `r format(as.Date(largest.increase), "%B %Y")` and it was higher by `r round(higher.percentage * 100, 2)`%. This was right before the pandemic and the increase is not as severe as the pandemic decrease was.

```{r comparing covid boom and bust impacts on fe model}
# Source
# U.S. Bureau of Economic Analysis, Population [POPTHM], 
# Retrieved from FRED, Federal Reserve Bank of St. Louis; 
# https://fred.stlouisfed.org/series/POPTHM, December 3, 2022.
# Pop Units in thousands
us.pop.month <- 
  readr::read_csv("./data/POPTHM.csv", show_col_types = FALSE) %>%
  rename(total_population = POPTHM, date = DATE) %>%
  mutate(date = as.Date(date, "%m/%d/%Y"))

# Filter to get population on date with largest increase
max_pop <- us.pop.month %>%
  filter(date == largest.increase)

# Filter to get population on date with largest decrease
min_pop <- us.pop.month %>%
  filter(date == largest.decrease)

# Calculate increase/decrease per capita based on min/max drive values and pop.
max_month_pop <- max_pop$total_population * 1000 # Units in thousands
min_month_pop <- min_pop$total_population * 1000
max_miles <- max * 1000000 # Units in millions
min_miles <- min * 1000000

max_miles_driven_pc <- max_miles / max_month_pop
min_miles_driven_pc <- min_miles / min_month_pop
  
# fatal rate - represents increases/decreases in the fatality rate
fatal_rate_covid_max <- round(coef(expanded.within)['miles_driven_per_capita']
                              * max_miles_driven_pc, 3) 
fatal_rate_covid_min <- round(coef(expanded.within)['miles_driven_per_capita']
                              * min_miles_driven_pc, 3)

```

The FE model suggests that a one mile increase in the miles driven per capita is associated with, ceteris paribus, a `r format(round(coef(expanded.within)['miles_driven_per_capita'], 5), scientific = FALSE)` log increase in the fatality rate. During the COVID boom, the largest increase in driving was on `r format(as.Date(largest.increase), "%B %Y")` and it results, ceteris paribus, in an estimated increase of `r round(max_miles_driven_pc, 2)` miles driven per capita and this is expected to result in a `r fatal_rate_covid_max` log increase in the fatality rate.

In contrast, the largest decrease in driving was on `r format(as.Date(largest.decrease), "%B %Y")` and it results, ceteris paribus, in an estimated decrease of `r round(min_miles_driven_pc, 2)` miles driven per capita and this is expected to result in a `r fatal_rate_covid_min` log decrease in the fatality rate.

# (5 points) Evaluate Error

#### Status incomplete

<!--
If there were serial correlation or heteroskedasticity in the idiosyncratic errors of the model, what would be the consequences on the estimators and their standard errors? Is there any serial correlation or heteroskedasticity? 
-->

## Analysis of Residuals

We test the residuals for serial correlation. The null hypothesis for the test is that the residuals are not serially correlated and the alternate hypothesis is that the residuals are serially correlated. We check against an $\alpha$ of 0.05.

```{r durbin watson test for serial correlation}
pwd <- pdwtest(expanded.within)
```

With a p-value of `r pwd$p.value`, we reject the null hypothesis of no serial correlation. This suggests that we must use robust standard errors for model parameters.

## Hetroskedasticity Test

We perform a Breusch Pagan test for heteroskedasticity. The null hypothesis for the test is that the residuals are homoskedastic and the alternate hypothesis is that the residuals are hetroskedastic. We check against an $\alpha$ of 0.05.

```{r breusch pagan test for heteroskedasticity}
pcd <- pcdtest(expanded.within, test = "lm")
```

With a p-value of `r pcd$p.value`, we reject the null hypothesis of homoskedasticity. This suggests that heteroscedasticity is present and our residuals are not distributed with equal variance. We may be able to correct for this by using heteroskedasticity-consistent standard errors or robust standard errors.' 

We compare the results of the Durbin Watson and Breusch-Godfrey test with order = 2. The null hypothesis is that there is no serial correlation of any order less than or equal to p. 

```{r compare watson breusch model with order for serial correlation}
pbg <- pbgtest(expanded.within, order = 2)
```

With a p-value of `r pbg$p.value`, we reject the null hypothesis and conclude that autocorrelation exists among the residuals at some order less than or equal to p.

Using vcovHC, we calculate robust standard errors (white1), cluster robust standard errors (white2), arellano standard errors (arellano), and newey west standard errors (using vcovNW) for the coefficients within our FE model.

```{r vcovhc test for robust standard erros}
reg.se <- coef(summary(expanded.within))[1,2]
het.se <- sqrt(vcovHC(expanded.within, method = "white1", type = "HC0")[1,1])
cluster.se <- sqrt(vcovHC(expanded.within, method = "white2", type = "HC0")[1,1])
nw.se <- sqrt(vcovNW(expanded.within, type = "HC0", maxlag = 1)[1,1])
arrellano.se <- sqrt(vcovHC(expanded.within, method = "arellano", type = "HC0")[1,1])

data.frame(
  "Type" = c("Regular OLS", "Robust", "Cluster Robust", "Newey West", "Arellano"),
  "SE" = c(reg.se, het.se, cluster.se, nw.se, arrellano.se)
)
```

Our analysis shows that the residuals in our model are both heterskedastic and have serial correlation. The results from the vcovHC analysis suggests that cluster robust standard errors are the most robust. Specifically, the standard errors for cluster robust sandard errors are slightly smaller than the regular OLS errors and robust standard error, while quite a bit higher (e.g, greater than 0.003) compared to the other standard error types.

```{r echo=FALSE, results='asis'}
pool.model.se <-sqrt(diag(vcovHC(pooled_ols, method = "white2", type = "HC0")))
exp.ols.model.se <-sqrt(diag(vcovHC(expanded.ols, method = "white2", type = "HC0")))
within.model.se <-sqrt(diag(vcovHC(expanded.within, method = "white2", type = "HC0")))
 
stargazer(pooled_ols, expanded.ols, expanded.within,
          se=list(pool.model.se, exp.ols.model.se, within.model.se), 
          style="qje", omit.stat=c("adj.rsq","f"), header = FALSE,
          column.labels = c( "Pooled", "Expanded OLS", "Within"))

```

When using robust cluster standard errors, our analysis shows that the fixed effects model has the highest R-squared value among the pooled and expanded OLS counterparts, as well as the lowest standard errors.
